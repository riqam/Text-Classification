{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "X_train = joblib.load('Test Kompas\\X_train_clean.pkl')\n",
    "X_valid = joblib.load('Test Kompas\\X_valid_clean.pkl')\n",
    "X_test = joblib.load('Test Kompas\\X_test_clean.pkl')\n",
    "\n",
    "y_train = joblib.load('Test Kompas\\y_train.pkl')\n",
    "y_valid = joblib.load('Test Kompas\\y_valid.pkl')\n",
    "y_test = joblib.load('Test Kompas\\y_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[['FullText']]\n",
    "X_valid = X_valid[['FullText']]\n",
    "X_test = X_test[['FullText']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Tentukan ukuran vocabulary dan panjang sequence maksimum\n",
    "vocab_size = 5000  # Sesuaikan dengan data Anda\n",
    "sequence_length = 300  # Sesuaikan dengan panjang rata-rata teks Anda\n",
    "\n",
    "# Buat layer TextVectorization\n",
    "vectorize_layer = TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)\n",
    "\n",
    "# Lakukan adaptasi pada data pelatihan untuk membuat vocabulary\n",
    "text_ds = tf.data.Dataset.from_tensor_slices(X_train['FullText']).batch(128)\n",
    "vectorize_layer.adapt(text_ds)\n",
    "\n",
    "# Buat model sequential\n",
    "model = tf.keras.Sequential([\n",
    "    vectorize_layer,  # Layer vektorisasi\n",
    "    layers.Embedding(vocab_size, 128, mask_zero=True),  # Layer embedding\n",
    "    layers.Bidirectional(layers.LSTM(64)),  # Layer LSTM dua arah\n",
    "    layers.Dense(64, activation='relu'),  # Layer dense\n",
    "    layers.Dense(len(np.unique(y_train)), activation='softmax')  # Output layer dengan softmax\n",
    "])\n",
    "\n",
    "# Kompilasi model\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Inisialisasi LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Fit dan transform label menjadi nilai numerik\n",
    "train_labels = le.fit_transform(y_train)\n",
    "valid_labels = le.transform(y_valid)\n",
    "\n",
    "# Konversi data teks dan label menjadi dataset TensorFlow\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train['FullText'], train_labels)).batch(32)\n",
    "valid_ds = tf.data.Dataset.from_tensor_slices((X_valid['FullText'], valid_labels)).batch(32)\n",
    "\n",
    "# Latih model\n",
    "history = model.fit(train_ds, validation_data=valid_ds, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 181ms/step - accuracy: 0.2047 - loss: 2.9683 - val_accuracy: 0.2550 - val_loss: 2.6153\n",
      "Epoch 2/15\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 161ms/step - accuracy: 0.2668 - loss: 2.3618 - val_accuracy: 0.3050 - val_loss: 2.1932\n",
      "Epoch 3/15\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 165ms/step - accuracy: 0.3657 - loss: 2.0061 - val_accuracy: 0.3850 - val_loss: 2.1982\n",
      "Epoch 4/15\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 160ms/step - accuracy: 0.5040 - loss: 1.7512 - val_accuracy: 0.4550 - val_loss: 1.9647\n",
      "Epoch 5/15\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 158ms/step - accuracy: 0.5975 - loss: 1.3594 - val_accuracy: 0.5125 - val_loss: 1.7332\n",
      "Epoch 6/15\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 159ms/step - accuracy: 0.7000 - loss: 1.0382 - val_accuracy: 0.5475 - val_loss: 1.7349\n",
      "Epoch 7/15\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 158ms/step - accuracy: 0.7632 - loss: 0.8050 - val_accuracy: 0.5050 - val_loss: 1.9496\n",
      "Epoch 8/15\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 158ms/step - accuracy: 0.7667 - loss: 0.7672 - val_accuracy: 0.4900 - val_loss: 2.2556\n",
      "Epoch 9/15\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 159ms/step - accuracy: 0.8378 - loss: 0.5486 - val_accuracy: 0.5250 - val_loss: 2.1833\n",
      "Epoch 10/15\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 162ms/step - accuracy: 0.8607 - loss: 0.4888 - val_accuracy: 0.5600 - val_loss: 1.9546\n",
      "Epoch 11/15\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 160ms/step - accuracy: 0.8689 - loss: 0.4017 - val_accuracy: 0.5350 - val_loss: 2.2144\n",
      "Epoch 12/15\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 164ms/step - accuracy: 0.9037 - loss: 0.3028 - val_accuracy: 0.5450 - val_loss: 2.2555\n",
      "Epoch 13/15\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 162ms/step - accuracy: 0.9016 - loss: 0.3270 - val_accuracy: 0.5850 - val_loss: 2.1382\n",
      "Epoch 14/15\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 161ms/step - accuracy: 0.9485 - loss: 0.1923 - val_accuracy: 0.5675 - val_loss: 2.1992\n",
      "Epoch 15/15\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 161ms/step - accuracy: 0.9684 - loss: 0.1388 - val_accuracy: 0.5800 - val_loss: 2.2409\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - accuracy: 0.6116 - loss: 1.9987\n",
      "Loss: 2.2408995628356934\n",
      "Accuracy: 0.5799999833106995\n"
     ]
    }
   ],
   "source": [
    "# Evaluasi model\n",
    "loss, accuracy = model.evaluate(valid_ds)\n",
    "print(f'Loss: {loss}')\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 341ms/step - accuracy: 0.1538 - loss: 2.9310 - val_accuracy: 0.2575 - val_loss: 2.4529\n",
      "Epoch 2/15\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 320ms/step - accuracy: 0.2893 - loss: 2.1215 - val_accuracy: 0.3575 - val_loss: 2.2657\n",
      "Epoch 3/15\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 323ms/step - accuracy: 0.5690 - loss: 1.4739 - val_accuracy: 0.4600 - val_loss: 1.8648\n",
      "Epoch 4/15\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 317ms/step - accuracy: 0.6832 - loss: 1.0927 - val_accuracy: 0.4175 - val_loss: 2.0082\n",
      "Epoch 5/15\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 316ms/step - accuracy: 0.7331 - loss: 0.8615 - val_accuracy: 0.4550 - val_loss: 2.0959\n",
      "Epoch 6/15\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 314ms/step - accuracy: 0.7789 - loss: 0.7134 - val_accuracy: 0.5200 - val_loss: 2.0066\n",
      "Epoch 7/15\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 317ms/step - accuracy: 0.8479 - loss: 0.5283 - val_accuracy: 0.5375 - val_loss: 2.0350\n",
      "Epoch 8/15\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 317ms/step - accuracy: 0.8888 - loss: 0.3719 - val_accuracy: 0.5450 - val_loss: 2.0871\n",
      "Epoch 9/15\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 315ms/step - accuracy: 0.9160 - loss: 0.2759 - val_accuracy: 0.5225 - val_loss: 2.2114\n",
      "Epoch 10/15\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 315ms/step - accuracy: 0.9520 - loss: 0.1881 - val_accuracy: 0.5400 - val_loss: 2.3318\n",
      "Epoch 11/15\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 313ms/step - accuracy: 0.9657 - loss: 0.1383 - val_accuracy: 0.5475 - val_loss: 2.4208\n",
      "Epoch 12/15\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 313ms/step - accuracy: 0.9732 - loss: 0.1015 - val_accuracy: 0.5650 - val_loss: 2.3930\n",
      "Epoch 13/15\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 318ms/step - accuracy: 0.9842 - loss: 0.0679 - val_accuracy: 0.5175 - val_loss: 2.7571\n",
      "Epoch 14/15\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 321ms/step - accuracy: 0.9914 - loss: 0.0412 - val_accuracy: 0.5600 - val_loss: 2.6262\n",
      "Epoch 15/15\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 326ms/step - accuracy: 0.9997 - loss: 0.0196 - val_accuracy: 0.5900 - val_loss: 2.5771\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Tentukan ukuran vocabulary dan panjang sequence maksimum\n",
    "vocab_size = 5000  # Sesuaikan dengan data Anda\n",
    "sequence_length = 300  # Sesuaikan dengan panjang rata-rata teks Anda\n",
    "\n",
    "# Buat layer TextVectorization\n",
    "vectorize_layer = TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)\n",
    "\n",
    "# Lakukan adaptasi pada data pelatihan untuk membuat vocabulary\n",
    "text_ds = tf.data.Dataset.from_tensor_slices(X_train['FullText']).batch(128)\n",
    "vectorize_layer.adapt(text_ds)\n",
    "\n",
    "# Buat model sequential\n",
    "model = tf.keras.Sequential([\n",
    "    vectorize_layer,  # Layer vektorisasi\n",
    "    layers.Embedding(vocab_size, 128, mask_zero=True),  # Layer embedding\n",
    "    layers.Bidirectional(layers.LSTM(64, return_sequences=True)),  # Layer LSTM dua arah\n",
    "    layers.Dropout(0.5),  # Tambahkan lapisan dropout\n",
    "    layers.Bidirectional(layers.LSTM(64)),  # Layer LSTM dua arah\n",
    "    layers.Dense(64, activation='relu'),  # Layer dense\n",
    "    layers.Dense(len(np.unique(y_train)), activation='softmax')  # Output layer dengan softmax\n",
    "])\n",
    "\n",
    "# Kompilasi model\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Inisialisasi LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Fit dan transform label menjadi nilai numerik\n",
    "train_labels = le.fit_transform(y_train)\n",
    "valid_labels = le.transform(y_valid)\n",
    "\n",
    "# Konversi data teks dan label menjadi dataset TensorFlow\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train['FullText'], train_labels)).batch(32)\n",
    "valid_ds = tf.data.Dataset.from_tensor_slices((X_valid['FullText'], valid_labels)).batch(32)\n",
    "\n",
    "# Latih model\n",
    "history = model.fit(train_ds, validation_data=valid_ds, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
